{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选取电影评论80000条，进行数据处理，删除无效数据，根据star对数据进行情感分类，star大于等于4，代表喜欢1；否则为不喜欢0。所以本问题，是个二分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(r'/Users/yang/Desktop/get_douban_comments-master/movie_comments.csv',encoding=\"utf-8\")\n",
    "file_1=file.iloc[:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>id</td>\n",
       "      <td>link</td>\n",
       "      <td>name</td>\n",
       "      <td>comment</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  link  name  comment  star\n",
       "568  id  link  name  comment  star"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_1[file_1['star']=='star'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yang/Library/Python/3.6/lib/python/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, link, name, comment, star]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_1.drop([568],inplace=True)\n",
    "file_1[file_1['star']=='star'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "file_1['sentiment']=[1 if int(file_1.iloc[i]['star'])>=4 else 0 for i in range(len(file_1))]\n",
    "file_2=file_1.drop(['link','name','star','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_2.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 34691, 1: 45308})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(file_2['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4336429205365067, 0.5663570794634933)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34691/(34691+45308),1-34691/(34691+45308)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上可见，两种类型的标签数量都在50%左右。此问题并非垃圾邮件之类的“偏斜类问题”。可以使用accuracy来进行模型效果衡量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords\n",
    "stopwords=stopwordslist('/Users/yang/Desktop/stopwords.txt')\n",
    "\n",
    "comments=[list(jieba.cut(str(i))) for i in file_2['comment']]\n",
    "\n",
    "def get_corpus(comments,stopwords):\n",
    "    corpus=[]\n",
    "    for element in comments:\n",
    "        sentence=[]\n",
    "        for word in element:\n",
    "            if word not in stopwords and (word not in [' ','，','。']):\n",
    "                sentence.append(word)\n",
    "        corpus.append(sentence)  \n",
    "    return corpus\n",
    "corpus=get_corpus(comments,stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上，由于电影评论的很多标点符号是英文符号，所以'/Users/yang/Desktop/stopwords.txt'中既有中文停词，又有英文停词。将评论内容进行切词，形成2维的list--comments，以便之后将词替换为数字。对comments中的词进行去停词处理，获得二维的list--corpus。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length of sentence: 159\n",
      "number of words: 981589\n",
      "number of set(words): 81488\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length=max([(len(i)) for i in comments])\n",
    "words=list(itertools.chain.from_iterable(corpus))\n",
    "print('max_length of sentence:',max_sentence_length)\n",
    "print('number of words:',len(words))\n",
    "print('number of set(words):',len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上， 将corpus由二维list变为1维的list---words，即电影评论中所有词的数量为981589，去除重复之后的词汇量为81488，句子的最大长度为159。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不错\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 30000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "word2index, index2word = build_dataset(words)\n",
    "\n",
    "print(index2word[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1000):\n",
    "#   print(index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(corpus,word2index):\n",
    "    data = []\n",
    "    for i in corpus:\n",
    "        sequence=[]\n",
    "        for word in i:\n",
    "            if word in word2index:\n",
    "                sequence.append(word2index[word])\n",
    "            else:\n",
    "                sequence.append(0)\n",
    "        data.append(sequence)\n",
    "    return data\n",
    "data=get_data(corpus,word2index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上，构建词汇索引表。\n",
    "对于words中的81488个词，出现频率最高的前29999个词，按照频率由高到低的次序，编号为1----29999，其余词的编号都为0。生成word2index, index2word字典。然后，根据corpus，生成data--将corpus中的词替换为相应数字编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100\n",
    "X=sequence.pad_sequences(data,maxlen=maxlen,padding=\"pre\",value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=file_2['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上，将句子的长度进行统一处理为100，少的补0；多的截取前面部分。再进行数据集的拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE =64\n",
    "MAX_FEATURES=90000\n",
    "\n",
    "\n",
    "vocab_size=min(MAX_FEATURES, len(set(words))) + 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=maxlen))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上，因为是二分类问题，loss参数选\"binary_crossentropy\"，Activation选\"sigmoid\"。\n",
    "经过对vocabulary_size，dropout，以及评论数量的调整。当评论数量为80000时，最好的结果为vocabulary_size=30000，dropout=0.2。vocabulary_size过大或过小。准确率都会大大降低至50%。猜想原因：vocabulary_size过小，可能会将重要词汇忽略，而过大，将不能识别重要词汇。由于每次训练花费时间较长，效果仍在改进中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断模型效果的方法：learning rate曲线。\n",
    "当随着训练样本增多，训练集的准确率很高，测试集的准确率较低，且二者之间有明显差距时，为overfitting。需要减少feature，dropout、regularizarion、早停或者增加训练样本数量。\n",
    "当随着训练样本增多，训练集的准确率较低，测试集的准确率较低，且二者达到接近时，为underfitting。需要选择新的特征，或者使用更复杂的模型等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63999 samples, validate on 16000 samples\n",
      "Epoch 1/10\n",
      "63999/63999 [==============================] - 422s 7ms/step - loss: 0.5444 - acc: 0.7206 - val_loss: 0.4905 - val_acc: 0.7655\n",
      "Epoch 2/10\n",
      "63999/63999 [==============================] - 421s 7ms/step - loss: 0.4046 - acc: 0.8145 - val_loss: 0.4916 - val_acc: 0.7722\n",
      "Epoch 3/10\n",
      "63999/63999 [==============================] - 420s 7ms/step - loss: 0.3186 - acc: 0.8597 - val_loss: 0.5301 - val_acc: 0.7697\n",
      "Epoch 4/10\n",
      "63999/63999 [==============================] - 415s 6ms/step - loss: 0.2507 - acc: 0.8896 - val_loss: 0.6089 - val_acc: 0.7692\n",
      "Epoch 5/10\n",
      "63999/63999 [==============================] - 415s 6ms/step - loss: 0.2021 - acc: 0.9091 - val_loss: 0.6963 - val_acc: 0.7693\n",
      "Epoch 6/10\n",
      "63999/63999 [==============================] - 415s 6ms/step - loss: 0.1678 - acc: 0.9259 - val_loss: 0.8195 - val_acc: 0.7686\n",
      "Epoch 7/10\n",
      "63999/63999 [==============================] - 415s 6ms/step - loss: 0.1418 - acc: 0.9367 - val_loss: 0.8997 - val_acc: 0.7681\n",
      "Epoch 8/10\n",
      "63999/63999 [==============================] - 415s 6ms/step - loss: 0.1256 - acc: 0.9448 - val_loss: 0.9661 - val_acc: 0.7669\n",
      "Epoch 9/10\n",
      "63999/63999 [==============================] - 416s 6ms/step - loss: 0.1085 - acc: 0.9525 - val_loss: 1.0269 - val_acc: 0.7643\n",
      "Epoch 10/10\n",
      "63999/63999 [==============================] - 421s 7ms/step - loss: 0.0983 - acc: 0.9558 - val_loss: 1.1353 - val_acc: 0.7689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ab0ce10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
